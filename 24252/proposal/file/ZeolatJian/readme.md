<a href="https://github.com/drshahizan/research-design/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/research-design" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/research-design/network/members"><img src="https://img.shields.io/github/forks/drshahizan/research-design" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/research-design/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/research-design" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/research-design"><img src="https://img.shields.io/github/issues/drshahizan/research-design" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/research-design/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/research-design?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2BDM&labelColor=%23d9e3f0&countColor=%23697689&style=flat)

<p align="center">
  <img height="300px" src="img/photo 1.jpg" alt="Profile Image">
</p>

<table align="center">
  <tr>
    <th>Name</th>
    <th>Matric No.</th>
  </tr>
  <tr>
    <td>LEE HONG JIAN</td>
    <td>MCS241054</td>
  </tr>
</table>

# [Reinforcement Learning for Automated Trading in Stock Market]

## Files

| No  | Chapter     |                                                 File |
| :-: | ---------- | :---------------------------------------------------------------------------------------------------: |
|  1.  | Proposal | <a href="proposal/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  2.  | Chapter 1 | <a href="c1/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  3.  | Chapter 2 | <a href="c2/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  4.  | Chapter 3 | <a href="c3/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  5.  | Chapter 4 | <a href="c4/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  6.  | Chapter 5 | <a href="c5/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  7.  | Complete Chapter | <a href="Full Chapter/"><img src="img/pdf.svg" width="24px" height="24px"></a> |
|  8.  | Code | <a href="code"><img src="img/python_icon.png" width="24px" height="24px"></a> |


## Table of Contents
- [Abstract](#abstract)
- [Research Objectives](#research-objectives)
- [Scope of Work](#scope-of-work)
- [Methodology](#methodology)
- [Expected Outcomes](#expected-outcomes)

## Abstract

The purpose of the research is to train agent that will not be influenced by the sentimental with using deep reinforcement learning (DRL) model in trading. In the reality of the financial market, the traditional trading strategy is based on the analysis (technical and fundamental) and statistical models to make decision, however, the limitation of traditional trading strategy also significant which when dueling with the highly dynamics of complex market. Nowadays, the main problem of the trading is the human emotional biases which is affected in the decision making and decrease the trading strategy of effective. In trading field, DRL was playing the importance roles in decision making because its will not distributed by emotional. An experienced trader may be affected by emotional by decision making however DRL can totally prevent the incident happen. The high potential of DRL in trading especially due to the dynamics market floating. DRL is one of the solutions for the emotional biases in financial markets. Compare to original/traditional systems, DRL algorithms able to learn directly from the actual market interactions and able to adjust the strategy to maximize the return rate based on the dynamics market. The learning of market interaction allows agents to improve the decision making and maximize the return rate. The aim is to create the policy that can maximize the cumulative reward over time. The suitable policy and reward mechanism is needed to train the agent that can fully eliminated the sentimental and the decision-making will be more discipline and the profit will be optimized. Every model has their own strengths and weaknesses; by identify the advantages of model will let the stakeholder apply those models in more appropriately toward the real-time market environment. The model with the higher value in F1-scores and accuracy, and optimized cumulative return rates based on the back testing will be selected for the future trading strategy in decision making. , DRL are able to working consistent and well in dynamics market environment such as the Two Sigma in DRL where the system will keep adapts the new various high amount of data and identify the profitable strategy that traditional trading strategy that unable to make it.  The high profitability and adaptability without influenced by sentimental factor in the highly dynamics market environment.


## Keywords

Deep Reinforcement Learning, Stock Market, Automated Trading, Decision-Making, Sentimental Biases, DQN, PPO, SAC

## Research Objectives

1. To obtain the policy that give optimized return.
2. To train agent that will not be influenced by the sentimental with using DRL model.
3. Develop a dashboard that visualize the return of the agent that trained on different mechanism.
4. Compare the performance within model and discuss the strength and weakness between different model (DQN, PPO and SAC).

## Scope of Work
- Computing tools (Python) will be used for the data collection and process in the research project. 
- The process of cleaning data will be conducted which is cleared the missing data and prepare work for the descriptive analysis.
- The DQN, PPO and SAC will be used to do the decision making based on the policy and reward mechanism that can achieve the optimized return profit.
- The comparison of performance between model will using F1-score and optimized cumulative return rates.
- The strengths and weaknesses of the model also will be discussed based on the performance.

## Methodology

1. **Data Collection:**
   - The range of time series data of the standard & poor 500 (S & P 500) from 01/01/2016 to 01/01/2024 will be collected from Yahoo Finance.

2. **Data Analysis:**
   - The DQN, PPO and SAC will be used to do the decision making

3. **Validation:**
   -  Based on the policy and reward mechanism that can achieve the optimized return profit.

## Expected Outcomes

In the research, the expected contribution is to train agent with the high efficiency and stable in sentimental. What kinds of the difference will be brought by using the different model in trading agents which is PPO, SAC and DQN. Indicate the strengths and weaknesses of the PPO, SAC and DQN in trading strategy. Helping stakeholder to harvest more profit without worry and sentimental impact. The trader will be automation decision making based on the policy and reward mechanism. The establish policy and reward mechanism is needed to train the agent that can fully eliminated the sentimental and the decision-making will be more discipline and the profit will be optimized. Every model has their own strengths and weaknesses; by identify the advantages of model will let the stakeholder apply those models in more appropriately toward the real-time market environment. The model with the higher value in F1-scores and accuracy, and optimized cumulative return rates based on the back testing will be selected for the future trading strategy in decision making.

*For inquiries, contact: [leehongjian.email@utm.my]*

 




## Contribution üõ†Ô∏è
Please create an [Issue](https://github.com/drshahizan/research-design/issues) for any improvements, suggestions or errors in the content.

[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan)
![](https://hit.yhype.me/github/profile?user_id=81284918)

